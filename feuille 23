base<-read.csv2("D:/moste/OneDrive/Bureau/LAKOUK.csv",  header= T, check.names = F)
View(base)## refers to our dataframe that contains a value of each parametres
###step 1: consist to select only the most important variables using PCA###
## PACKAGES for PCA visualisation:###
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggrepel)
variables<-base
res.pca <- PCA(variables, graph = FALSE)
res.pca
fviz_eig(res.pca)
res.pca <- PCA(variables, graph = FALSE)
###First of all we should select the retained dimensions of PCA:###
eig.val <- get_eigenvalue(res.pca)###using the get_eignvalue function###
eig.val ### from the KAISER rule we should take into consideration only the dimensions that have eigenvalue grather than 1(in this case we have 4 dimensions)

### after the prcess of selction of dimension we should select only the 2 most important variables that contribute on the formation of  the fourth axes (2 that have the higher value of contribution for each axes)##
### we need also to visualise the correlation matrix 
###check the correltaion between parametres 
cor_matrix <- cor(variables)
# Visualization of the correlation matrix for more detailed insights and information:
print(cor_matrix)
library(corrplot)
corrplot(cor_matrix, method = "number")
corrplot(cor_matrix, method = "circle")
###extract the most important variables of each dimension
var <- get_pca_var(res.pca)
var
corrplot(var$contrib, is.corr=FALSE) 
var$contrib
### We should consider the correlation matrix to eliminate variables that have high correlation(grather than 0.7) values with each other.
fviz_contrib(res.pca, choice = "var", axes = 1)###only the CE have selected(we select the first parametres (variables) following their contrib value in formation of the first dim(axe), but we have eliminated the TH and CaCO3 for the reason of higher correlation )
fviz_contrib(res.pca, choice = "var", axes = 2)###The saturation rate, O2 dissolved, and turbidity (which replaced the second most important variables in dimension 1) were selected based on their contribution values in forming the second dimension (axis). We replaced turbidity in place of TH (hardness) based on this selection
fviz_contrib(res.pca, choice = "var", axes = 3)###NH4 and PO43 is selected (we select the 2 first parametres (variables) following their contrib value in formation of the third dim(axe))
fviz_contrib(res.pca, choice = "var", axes = 4)###NO3 and NO2 were selected because PO43 is found in the third dimension, and turbidity (which occupies the third position) was also selected. Therefore, we chose the third most important variable in the third dimension, which is nitrite (NO2). So, we selected the first two parameters based on their contribution values in forming the fourth dimension (axis)
##the total parametres selected is (CE, saturation rate, O2 dissolved,  turbidity, NH4, PO43, NO3 and NO2)
###the contribution value of each parametres for the fourth top dimension
var$contrib["CE",1:4]
var$contrib["Saturation",1:4]
var$contrib["O2_Dissous",1:4]
var$contrib["Tur",1:4]
var$contrib["NH4",1:4]
var$contrib["PO43",1:4]
var$contrib["NO3",1:4]
var$contrib["NO2",1:4]
##After the process of selecting the 8 parameters, we need to calculate the Weighting Arithmetic Water Quality Index (WA_WQI)
base<-read.csv2("D:/moste/OneDrive/Bureau/IQE1.csv",  header= T, check.names = F)
base1<-base[1:159,]##This dataframe contains the results of all 8 parameters across all stations in the study. 
base2<-base[160:318,]##refers to the standard limit of each paramteres
head(base1)
### the standar limitaiton of the 8 parametres:NO2	CE	Tur	NO3	NH4	Saturation	PO43	O2_Dissous
# standard value for each paramÃ¨tre
NO2 <- 0.03
CE <- 2500
Tur <- 1
NO3 <- 50
NH4 <- 0.5
Saturation <- 70
PO43 <- 0.1
O2_Dissous <- 7
df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base2<-df
head(base1)###results value of 8 parametres selected
head(base2)###base2 represnt the standard value of each paramaters
###Step 2: calculating the WA_WQI
S<-1/base2
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K
WI<-K/base2
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
View(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED  
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous<-data.frame(O2_Dissous)
head(O2_Dissous)
# add O2_Dissous as eithenth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
AW_WQI<-q1*WI
AW_WQI
sum(AW_WQI[1, ], na.rm = TRUE)##summarize per sample
AW_WQI <- rowSums(AW_WQI, na.rm = TRUE)
summary(AW_WQI)
#if you want transform as a data frame 
AW_WQI<-data.frame(AW_WQI)
#View(AW_WQI)
###if you need to excract the results 
#write.xlsx(AW_WQI, file="D:/moste/OneDrive/Bureau/AW_WQI.xlsx")
###We need to check multicollinearity using the vif function from the car package
base1$AW_WQI <-AW_WQI
names(base1)
library(car)
modele <- lm(AW_WQI ~NO2 + CE + Tur + NO3 + NH4 + Saturation + PO43 + O2_Dissous, data = base1)
vif_values <- vif(modele)
# Afficher les valeurs VIF
print(vif_values)###all thevif values as less than 5.
###new water quality index based on the calucation the index of each dimensions(axes) of PCA, the only differce is replaced the standards values by contribution value of formation of dimensions BUUUUUT WHEN WE CALCULATE ONLY THE WEITHED VALUE WI.
NO2<-var$contrib["NO2",1]
CE<-var$contrib["CE",1]##contrib value of CE to formation of the first axe 
Tur<-var$contrib["Tur",1]
NO3<-var$contrib["NO3",1]
NH4<-var$contrib["NH4",1]
Saturation<-var$contrib["Saturation",1]
PO43<-var$contrib["PO43",1]
O2_Dissous<-var$contrib["O2_Dissous",1]

df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base_D1<-df
View(base_D1)
###represents the contribution value of each parametres for the first dimension
###Step 2: calculating the WQI_D1
S<-1/base_D1
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K ###THE CONSTANT K
WI<-K/base_D1
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base1[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED 
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous
# add O2_Dissous as eighth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
WQI_D1<-q1*WI
WQI_D1
sum(WQI_D1[1, ], na.rm = TRUE)##summarize per sample
WQI_D1 <- rowSums(WQI_D1, na.rm = TRUE)
WQI_D1
WQI_D1<-data.frame(WQI_D1)
summary(WQI_D1)
write.xlsx(WQI_D1, file="D:/moste/OneDrive/Bureau/WQI_D1.xlsx")
aaaaaaaaaaa
###DIM2
NO2<-var$contrib["NO2",2]
CE<-var$contrib["CE",2]##contrib value of CE to formation of the first axe 
Tur<-var$contrib["Tur",2]
NO3<-var$contrib["NO3",2]
NH4<-var$contrib["NH4",2]
Saturation<-var$contrib["Saturation",2]
PO43<-var$contrib["PO43",2]
O2_Dissous<-var$contrib["O2_Dissous",2]

df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base_D2<-df
head(base_D1)###represents the contribution value of each parametres for the first dimension
###Step 2: calculating the WQI_D1
S<-1/base_D2
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K ###THE CONSTANT K
WI<-K/base_D2
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base1[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED 
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous
# add O2_Dissous as eighth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
WQI_D2<-q1*WI
WQI_D2
sum(WQI_D2[1, ], na.rm = TRUE)##summarize per sample
WQI_D2 <- rowSums(WQI_D2, na.rm = TRUE)
WQI_D2
WQI_D2<-data.frame(WQI_D2)
summary(WQI_D1)
write.xlsx(WQI_P, file="D:/moste/OneDrive/Bureau/WQI_P.xlsx")

p<-read.csv2("D:/moste/OneDrive/Bureau/MALAK2.csv")
names(p)
WQI_P<-((p$WQI_D1*47.20397823)/73.65070)+((p$WQI_D2*11.37620373)/73.65070)+((p$WQI_D3*8.28996521)/73.65070)+((p$WQI_D4*6.78055218)/73.65070)
WQI_P<-data.frame(WQI_P)
WQI_P

###DECISION TREE###
colnames(base1)[9] <-"WQI"
base1<-read.csv2("D:/moste/OneDrive/Bureau/IQE1.csv",  header= T, check.names = F)
library(tidyverse)
library(caret)
library(rpart)
# Inspect the data
sample_n(base1, 3)
# Split the data into training andje v test set
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
base1 <- na.omit(base1)
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]
set.seed(123)

dtmodelWQI<-model <- train(
  WQI   ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
print(dtmodelWQI)
plot(dtmodelWQI)
# Print the best tuning parameter cp that
# minimize the model RMSE
dtmodelWQI$bestTune
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(dtmodelWQI$finalModel)
text(dtmodelWQI$finalModel, digits = 3)
dtmodelWQI$finalModel
# Make predictions on the test data
predictions_DT <- dtmodelWQI %>% predict(test.data)
head(predictions_DT)
# Compute the prediction error RMSE
RMSE(predictions_DT, test.data$WQI)
R2(predictions_DT, test.data$WQI)
MAE(predictions_DT, test.data$WQI)
results_DT <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_DT
)
head(results_DT)
write.xlsx(results_DT, file="C:/Users/moste/OneDrive/Bureau/results_DT.xlsx")
###randoom forest###
base1 <- na.omit(base1)
set.seed(123)
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5, 6, 7, 8)  # Liste des valeurs Ã  tester pour mtry
)
train_control <- trainControl(method = "cv", number = 10)
rfmodelWQI <- train(
  WQI ~ ., 
  data = train.data, 
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid
)
# Best tuning parameter mtry
rfmodelWQI$bestTune
# Make predictions on the test data
predictions_RF <- rfmodelWQI %>% predict(test.data)
head(predictions_RF)
# Compute the average prediction error RMSE
RMSE(predictions_RF, test.data$WQI)
MAE(predictions_RF, test.data$WQI)
R2(predictions_RF, test.data$WQI)
results_RF <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_RF
)
head(results_RF)
write.xlsx(results_RF, file="C:/Users/moste/OneDrive/Bureau/results_RF.xlsx")
####ANN###
library(tidyverse)
library(neuralnet)
base1 <- na.omit(base1)
# CrÃ©er les Ã©chantillons d'entraÃ®nement et de test
set.seed(123)  # Fixer la graine pour la reproductibilitÃ©
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]

# DÃ©finir le contrÃ´le d'entraÃ®nement pour la validation croisÃ©e Ã  10 plis
train_control <- trainControl(method = "cv", number = 10)

# EntraÃ®ner le modÃ¨le ANN
annmodelWQI <- train(
  WQI ~ .,  # ModÃ©liser WQI en fonction de toutes les autres variables
  data = train.data,
  method = "nnet",  # Utiliser un rÃ©seau de neurones artificiel
  trControl = train_control,  # Validation croisÃ©e Ã  10 plis
  linout = TRUE,    # Pour la rÃ©gression (output linÃ©aire)
  trace = FALSE,    # Pour dÃ©sactiver les sorties de progression
  tuneLength = 5    # Essayer 5 combinaisons diffÃ©rentes de paramÃ¨tres
)

# Afficher les rÃ©sultats du modÃ¨le entraÃ®nÃ©
print(annmodelWQI)
annmodelWQI$bestTune
# Faire des prÃ©dictions sur les donnÃ©es de test
predictions_ANN <- annmodelWQI %>% predict(test.data)
head(predictions_ANN)
# Ãvaluer la performance du modÃ¨le sur les donnÃ©es de test
results_ANN <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_ANN
)

# Calculer le RMSE (Root Mean Squared Error) pour Ã©valuer les performances
rmse <- sqrt(mean((results_ANN$Observed - results_ANN$Predicted)^2))
# Afficher les rÃ©sultats
print(results_ANN)
print(paste("RMSE:", rmse))
RMSE(predictions_ANN, test.data$WQI)
MAE(predictions_ANN, test.data$WQI)
R2(predictions_ANN, test.data$WQI)
write.xlsx(results_ANN, file="C:/Users/moste/OneDrive/Bureau/results_ANN.xlsx")
###XGBOOST
library(tidyverse)
library(caret)
library(xgboost)
set.seed(123)  # Fixer la graine pour la reproductibilitÃ©
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]
train_control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(
  nrounds = c(50, 100),          # Nombre d'arbres (rÃ©duit Ã  2 valeurs)
  max_depth = c(3, 6),           # Profondeur maximale des arbres (rÃ©duit Ã  2 valeurs)
  eta = c(0.1, 0.3),             # Taux d'apprentissage (rÃ©duit Ã  2 valeurs)
  gamma = c(0, 1),               # Terme de rÃ©gularisation pour les branches (rÃ©duit Ã  2 valeurs)
  colsample_bytree = c(0.7),     # Proportion des colonnes Ã©chantillonnÃ©es (rÃ©duit Ã  1 valeur)
  min_child_weight = c(1, 5),    # Poids minimum pour les enfants (rÃ©duit Ã  2 valeurs)
  subsample = c(0.7)             # Proportion des Ã©chantillons utilisÃ©s pour construire chaque arbre (rÃ©duit Ã  1 valeur)
)

# Ajuster le modÃ¨le XGBoost avec les hyperparamÃ¨tres dÃ©finis dans la grille
xgbmodelWQI <- train(
  WQI ~ .,                         # ModÃ©liser WQI en fonction de toutes les autres variables
  data = train.data,               # DonnÃ©es d'entraÃ®nement
  method = "xgbTree",              # Utiliser XGBoost avec des arbres de dÃ©cision
  trControl = train_control,       # Validation croisÃ©e Ã  10 plis
  tuneGrid = tune_grid             # Grille d'hyperparamÃ¨tres Ã  tester
)
# Best tuning parameter mtry
xgbmodelWQI$bestTune
# Make predictions on the test data
predictions_XGB <- xgbmodelWQI %>% predict(test.data)
head(predictions_XGB)
# Compute the average prediction error RMSE
RMSE(predictions_XGB, test.data$WQI)
MAE(predictions_XGB, test.data$WQI)
R2(predictions_XGB, test.data$WQI)
results_XGB <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_XGB
)
head(results_XGB)
write.xlsx(results_XGB, file="C:/Users/moste/OneDrive/Bureau/results_XGB.xlsx")
###KNN NEIGHBORS###
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
model_KNN<-model <- train(
  WQI~., data = train.data, method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center","scale"),
  tuneLength = 10
)
# Plot model error RMSE vs different values of k
plot(model_KNN)
# Best tuning parameter k that minimize the RMSE
model_KNN$bestTune
# Make predictions on the test data
predictions_KNN <- model_KNN %>% predict(test.data)
head(predictions_KNN)
# Compute the prediction error RMSE
RMSE(predictions_KNN, test.data$WQI)
MAE(predictions_KNN, test.data$WQI)
R2(predictions_KNN, test.data$WQI)
results_KNN <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_KNN
)
head(results_KNN)
write.xlsx(results_KNN, file="C:/Users/moste/OneDrive/Bureau/results_KNN.xlsx")
###Predicted and Observed###
XGBP<-ggplot(results_XGB, aes(y=Predicted, x=Observed))+
  geom_point()+
  geom_smooth(colour="red", method="lm", fill="red") +
  ylab("Predicted WQI")+
  xlab("Actual WQI") +
  theme_classic()+
  annotate("text", x = 30, y = 80, label = "(RÂ²=0.97)")
ANannresultsWQI  
R2(predictions_KNN, test.data$WQI)
R2(predictions_XGB, test.data$WQI)
R2(predictions_RF, test.data$WQI)
R2(predictions_DT, test.data$WQI)
R2(predictions_ANN, test.data$WQI)




###DIAGRAM 
# Calculer l'Ã©cart-type et le coefficient de corrÃ©lation pour chaque modÃ¨le
std_dev_DT <- sd(predictions_DT)
corr_coef_DT <- cor(predictions_DT, test.data$WQI)

std_dev_RF <- sd(predictions_RF)
corr_coef_RF <- cor(predictions_RF, test.data$WQI)

std_dev_XGB <- sd(predictions_XGB)
corr_coef_XGB <- cor(predictions_XGB, test.data$WQI)

std_dev_KNN <- sd(predictions_KNN)
corr_coef_KNN <- cor(predictions_KNN, test.data$WQI)

std_dev_ANN <- sd(predictions_ANN)
corr_coef_ANN <- cor(predictions_ANN, test.data$WQI)
# CrÃ©er le diagramme de Taylor sans ajuster les limites
taylor.diagram(
  c(std_dev_DT, std_dev_RF, std_dev_XGB, std_dev_KNN, std_dev_ANN),
  c(corr_coef_DT, corr_coef_RF, corr_coef_XGB, corr_coef_KNN, corr_coef_ANN),
  pch=NA, col=NA,
  add=FALSE,
  main="Diagramme de Taylor"
)

# Ajuster les limites des axes aprÃ¨s la crÃ©ation du diagramme
par(new=TRUE)
plot(0, 0, type="n", xlim=c(0, 30), ylim=c(0, 30), xlab="", ylab="", axes=FALSE)
axis(1, at=c(0, 7.5, 15, 22.5, 30))
axis(2, at=c(0, 7.5, 15, 22.5, 30))

# Ajouter les points pour chaque modÃ¨le
points(std_dev_DT, corr_coef_DT, pch=1, col="red")
points(std_dev_RF, corr_coef_RF, pch=2, col="blue")
points(std_dev_XGB, corr_coef_XGB, pch=3, col="green")
points(std_dev_KNN, corr_coef_KNN, pch=4, col="purple")

# Ajouter une lÃ©gende
legend("topright", legend=c("DT", "RF", "XGB", "KNN"), pch=1:4, col=c("red", "blue", "green", "purple"))
###taylor diagram###
library(chron)
library(lattice)
library(ggplot2)
library(plotrix)
library(graphics)
###SUPER###
library(openair)
library(plorix)
RMSE(predictions_KNN, test.data$WQI)
RMSE(predictions_DT, test.data$WQI)
RMSE(predictions_RF, test.data$WQI)
RMSE(predictions_ANN, test.data$WQI)
RMSE(predictions_XGB, test.data$WQI)

std_dev_DT <- sd(predictions_DT)
corr_coef_DT <- cor(predictions_DT, test.data$WQI)

std_dev_RF <- sd(predictions_RF)
corr_coef_RF <- cor(predictions_RF, test.data$WQI)

std_dev_XGB <- sd(predictions_XGB)
corr_coef_XGB <- cor(predictions_XGB, test.data$WQI)

std_dev_KNN <- sd(predictions_KNN)
corr_coef_KNN <- cor(predictions_KNN, test.data$WQI)

std_dev_ANN <- sd(predictions_ANN)
corr_coef_ANN <- cor(predictions_ANN, test.data$WQI)


# Les donnÃ©es ObservÃ©es (observed) et les prÃ©dictions pour chaque modÃ¨le (pred_*)
observed <- test.data$WQI

# Diagramme de Taylor
taylor.diagram(observed, predictions_DT, add=FALSE, col="red", pch=19, main="Taylor Diagram", pos.cor=TRUE)
taylor.diagram(observed, predictions_RF, add=TRUE, col="blue", pch=19)
taylor.diagram(observed, predictions_ANN, add=TRUE, col="green", pch=19)
taylor.diagram(observed, predictions_XGB, add=TRUE, col="purple", pch=19)
taylor.diagram(observed, predictions_KNN, add=TRUE, col="orange", pch=19)

# Ajouter une lÃ©gende
legend("topright", legend=c("DT", "RF", "ANN", "XGB", "KNN"), col=c("red", "blue", "green", "purple", "orange"), pch=19)
sd(base1$WQI)


###STUCKING###
# CrÃ©er un dataframe avec les prÃ©dictions des modÃ¨les
stacking_data <- data.frame(
  DT = predictions_DT,
  RF = predictions_RF,
  ANN = predictions_ANN,
  XGB = predictions_XGB,
  KNN = predictions_KNN,
  Observed = test.data$WQI
)

# EntraÃ®ner le mÃ©ta-modÃ¨le
meta_model <- lm(Observed ~ ., data = stacking_data)

# Faire des prÃ©dictions avec le mÃ©ta-modÃ¨le
meta_predictions <- predict(meta_model, stacking_data)

# Ãvaluer la performance
meta_rmse <- RMSE(meta_predictions, test.data$WQI)
meta_corr <- cor(meta_predictions, test.data$WQI)
meta_sd <- sd(meta_predictions)

meta_rmse
meta_corr
meta_sd
###comment crÃ©er une Ã©quation et puis l'importer sur un fichier doc##
library(officer)
library(rmarkdown)
# CrÃ©er un document Word
doc <- read_docx()

# Ajouter des Ã©quations
doc <- doc %>%
  body_add_par("RMSE = sqrt(1/n * Î£(yi - Å·i)Â²)", style = "Normal") %>%
  body_add_par("MAE = 1/n * Î£ |yi - Å·i|", style = "Normal") %>%
  body_add_par("RÂ² = 1 - Î£(yi - Å·i)Â² / Î£(yi - È³)Â²", style = "Normal")

# Enregistrer le document
print(doc, target = "equations.docx")
