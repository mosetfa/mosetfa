base<-read.csv2("D:/moste/OneDrive/Bureau/LAKOUK.csv",  header= T, check.names = F)
View(base)## refers to our dataframe that contains a value of each parametres
###step 1: consist to select only the most important variables using PCA###
## PACKAGES for PCA visualisation:###
library(FactoMineR)
library(factoextra)
library(corrplot)
library(ggrepel)
variables<-base
res.pca <- PCA(variables, graph = FALSE)
res.pca
fviz_eig(res.pca)
res.pca <- PCA(variables, graph = FALSE)
###First of all we should select the retained dimensions of PCA:###
eig.val <- get_eigenvalue(res.pca)###using the get_eignvalue function###
eig.val ### from the KAISER rule we should take into consideration only the dimensions that have eigenvalue grather than 1(in this case we have 4 dimensions)

### after the prcess of selction of dimension we should select only the 2 most important variables that contribute on the formation of  the fourth axes (2 that have the higher value of contribution for each axes)##
### we need also to visualise the correlation matrix 
###check the correltaion between parametres 
cor_matrix <- cor(variables)
# Visualization of the correlation matrix for more detailed insights and information:
print(cor_matrix)
library(corrplot)
corrplot(cor_matrix, method = "number")
corrplot(cor_matrix, method = "circle")
###extract the most important variables of each dimension
var <- get_pca_var(res.pca)
var
corrplot(var$contrib, is.corr=FALSE) 
var$contrib
### We should consider the correlation matrix to eliminate variables that have high correlation(grather than 0.7) values with each other.
fviz_contrib(res.pca, choice = "var", axes = 1)###only the CE have selected(we select the first parametres (variables) following their contrib value in formation of the first dim(axe), but we have eliminated the TH and CaCO3 for the reason of higher correlation )
fviz_contrib(res.pca, choice = "var", axes = 2)###The saturation rate, O2 dissolved, and turbidity (which replaced the second most important variables in dimension 1) were selected based on their contribution values in forming the second dimension (axis). We replaced turbidity in place of TH (hardness) based on this selection
fviz_contrib(res.pca, choice = "var", axes = 3)###NH4 and PO43 is selected (we select the 2 first parametres (variables) following their contrib value in formation of the third dim(axe))
fviz_contrib(res.pca, choice = "var", axes = 4)###NO3 and NO2 were selected because PO43 is found in the third dimension, and turbidity (which occupies the third position) was also selected. Therefore, we chose the third most important variable in the third dimension, which is nitrite (NO2). So, we selected the first two parameters based on their contribution values in forming the fourth dimension (axis)
##the total parametres selected is (CE, saturation rate, O2 dissolved,  turbidity, NH4, PO43, NO3 and NO2)
###the contribution value of each parametres for the fourth top dimension
var$contrib["CE",1:4]
var$contrib["Saturation",1:4]
var$contrib["O2_Dissous",1:4]
var$contrib["Tur",1:4]
var$contrib["NH4",1:4]
var$contrib["PO43",1:4]
var$contrib["NO3",1:4]
var$contrib["NO2",1:4]
##After the process of selecting the 8 parameters, we need to calculate the Weighting Arithmetic Water Quality Index (WA_WQI)
base<-read.csv2("D:/moste/OneDrive/Bureau/IQE1.csv",  header= T, check.names = F)
base1<-base[1:159,]##This dataframe contains the results of all 8 parameters across all stations in the study. 
base2<-base[160:318,]##refers to the standard limit of each paramteres
head(base1)
### the standar limitaiton of the 8 parametres:NO2	CE	Tur	NO3	NH4	Saturation	PO43	O2_Dissous
# standard value for each paramètre
NO2 <- 0.03
CE <- 2500
Tur <- 1
NO3 <- 50
NH4 <- 0.5
Saturation <- 70
PO43 <- 0.1
O2_Dissous <- 7
df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base2<-df
head(base1)###results value of 8 parametres selected
head(base2)###base2 represnt the standard value of each paramaters
###Step 2: calculating the WA_WQI
S<-1/base2
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K
WI<-K/base2
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
View(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED  
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous<-data.frame(O2_Dissous)
head(O2_Dissous)
# add O2_Dissous as eithenth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
AW_WQI<-q1*WI
AW_WQI
sum(AW_WQI[1, ], na.rm = TRUE)##summarize per sample
AW_WQI <- rowSums(AW_WQI, na.rm = TRUE)
summary(AW_WQI)
#if you want transform as a data frame 
AW_WQI<-data.frame(AW_WQI)
#View(AW_WQI)
###if you need to excract the results 
#write.xlsx(AW_WQI, file="D:/moste/OneDrive/Bureau/AW_WQI.xlsx")
###We need to check multicollinearity using the vif function from the car package
base1$AW_WQI <-AW_WQI
names(base1)
library(car)
modele <- lm(AW_WQI ~NO2 + CE + Tur + NO3 + NH4 + Saturation + PO43 + O2_Dissous, data = base1)
vif_values <- vif(modele)
# Afficher les valeurs VIF
print(vif_values)###all thevif values as less than 5.
###new water quality index based on the calucation the index of each dimensions(axes) of PCA, the only differce is replaced the standards values by contribution value of formation of dimensions BUUUUUT WHEN WE CALCULATE ONLY THE WEITHED VALUE WI.
NO2<-var$contrib["NO2",1]
CE<-var$contrib["CE",1]##contrib value of CE to formation of the first axe 
Tur<-var$contrib["Tur",1]
NO3<-var$contrib["NO3",1]
NH4<-var$contrib["NH4",1]
Saturation<-var$contrib["Saturation",1]
PO43<-var$contrib["PO43",1]
O2_Dissous<-var$contrib["O2_Dissous",1]

df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base_D1<-df
View(base_D1)
###represents the contribution value of each parametres for the first dimension
###Step 2: calculating the WQI_D1
S<-1/base_D1
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K ###THE CONSTANT K
WI<-K/base_D1
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base1[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED 
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous
# add O2_Dissous as eighth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
WQI_D1<-q1*WI
WQI_D1
sum(WQI_D1[1, ], na.rm = TRUE)##summarize per sample
WQI_D1 <- rowSums(WQI_D1, na.rm = TRUE)
WQI_D1
WQI_D1<-data.frame(WQI_D1)
summary(WQI_D1)
write.xlsx(WQI_D1, file="D:/moste/OneDrive/Bureau/WQI_D1.xlsx")
aaaaaaaaaaa
###DIM2
NO2<-var$contrib["NO2",2]
CE<-var$contrib["CE",2]##contrib value of CE to formation of the first axe 
Tur<-var$contrib["Tur",2]
NO3<-var$contrib["NO3",2]
NH4<-var$contrib["NH4",2]
Saturation<-var$contrib["Saturation",2]
PO43<-var$contrib["PO43",2]
O2_Dissous<-var$contrib["O2_Dissous",2]

df <- data.frame(NO2, CE, Tur, NO3, NH4, Saturation, PO43, O2_Dissous)
print(df)
###We need to repeat the standard values to match the 159 rows in our results. 
df <- data.frame(
  NO2 = rep(NO2, 159),
  CE = rep(CE, 159),
  Tur = rep(Tur, 159),
  NO3 = rep(NO3, 159),
  NH4 = rep(NH4, 159),
  Saturation = rep(Saturation, 159),
  PO43 = rep(PO43, 159),
  O2_Dissous = rep(O2_Dissous, 159)
)
base_D2<-df
head(base_D1)###represents the contribution value of each parametres for the first dimension
###Step 2: calculating the WQI_D1
S<-1/base_D2
head(S)
ES<-sum(S[1, ], na.rm = TRUE)
ES
K<-1/ES
K ###THE CONSTANT K
WI<-K/base_D2
WI
sum(WI[1, ], na.rm = TRUE)##should be equal to 1
###We calculate the ratio of the value to the standard value, then multiply it by 100(there is an exception for O2_DISOLVED and PH(if is it selection).
q1<-base1[1:159,1:7]/base2[1:159,1:7]
q1<-q1*100
head(q1)
##if you want to extract the results...
#write.xlsx(q1, file="D:/moste/OneDrive/Bureau/q1.xlsx")
###this part is an exception for Ph but we dont need to run it only if we have selected PH##
#qPH<-base[1:159,(9)]-7#
#qPH<-qPH/2
#qPH<-qPH*100
#qPH
#qPH<-data.frame(qPH)
#qPH
#caculation of the q1 value of the exception parameters O2_DISOLVED 
O2_Dissous<-base1[1:159,(8)]-14.6
O2_Dissous<-O2_Dissous/(7-14.6)
O2_Dissous<-O2_Dissous*100
O2_Dissous
# add O2_Dissous as eighth colmn of q1
q1$O2_Dissous <- O2_Dissous
head(q1)
##applied the equation
WQI_D2<-q1*WI
WQI_D2
sum(WQI_D2[1, ], na.rm = TRUE)##summarize per sample
WQI_D2 <- rowSums(WQI_D2, na.rm = TRUE)
WQI_D2
WQI_D2<-data.frame(WQI_D2)
summary(WQI_D1)
write.xlsx(WQI_P, file="D:/moste/OneDrive/Bureau/WQI_P.xlsx")

p<-read.csv2("D:/moste/OneDrive/Bureau/MALAK2.csv")
names(p)
WQI_P<-((p$WQI_D1*47.20397823)/73.65070)+((p$WQI_D2*11.37620373)/73.65070)+((p$WQI_D3*8.28996521)/73.65070)+((p$WQI_D4*6.78055218)/73.65070)
WQI_P<-data.frame(WQI_P)
WQI_P

###DECISION TREE###
colnames(base1)[9] <-"WQI"
base1<-read.csv2("D:/moste/OneDrive/Bureau/IQE1.csv",  header= T, check.names = F)
library(tidyverse)
library(caret)
library(rpart)
# Inspect the data
sample_n(base1, 3)
# Split the data into training andje v test set
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
base1 <- na.omit(base1)
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]
set.seed(123)

dtmodelWQI<-model <- train(
  WQI   ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
print(dtmodelWQI)
plot(dtmodelWQI)
# Print the best tuning parameter cp that
# minimize the model RMSE
dtmodelWQI$bestTune
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(dtmodelWQI$finalModel)
text(dtmodelWQI$finalModel, digits = 3)
dtmodelWQI$finalModel
# Make predictions on the test data
predictions_DT <- dtmodelWQI %>% predict(test.data)
head(predictions_DT)
# Compute the prediction error RMSE
RMSE(predictions_DT, test.data$WQI)
R2(predictions_DT, test.data$WQI)
MAE(predictions_DT, test.data$WQI)
results_DT <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_DT
)
head(results_DT)
write.xlsx(results_DT, file="C:/Users/moste/OneDrive/Bureau/results_DT.xlsx")
###randoom forest###
base1 <- na.omit(base1)
set.seed(123)
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5, 6, 7, 8)  # Liste des valeurs à tester pour mtry
)
train_control <- trainControl(method = "cv", number = 10)
rfmodelWQI <- train(
  WQI ~ ., 
  data = train.data, 
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid
)
# Best tuning parameter mtry
rfmodelWQI$bestTune
# Make predictions on the test data
predictions_RF <- rfmodelWQI %>% predict(test.data)
head(predictions_RF)
# Compute the average prediction error RMSE
RMSE(predictions_RF, test.data$WQI)
MAE(predictions_RF, test.data$WQI)
R2(predictions_RF, test.data$WQI)
results_RF <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_RF
)
head(results_RF)
write.xlsx(results_RF, file="C:/Users/moste/OneDrive/Bureau/results_RF.xlsx")
####ANN###
library(tidyverse)
library(neuralnet)
base1 <- na.omit(base1)
# Créer les échantillons d'entraînement et de test
set.seed(123)  # Fixer la graine pour la reproductibilité
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]

# Définir le contrôle d'entraînement pour la validation croisée à 10 plis
train_control <- trainControl(method = "cv", number = 10)

# Entraîner le modèle ANN
annmodelWQI <- train(
  WQI ~ .,  # Modéliser WQI en fonction de toutes les autres variables
  data = train.data,
  method = "nnet",  # Utiliser un réseau de neurones artificiel
  trControl = train_control,  # Validation croisée à 10 plis
  linout = TRUE,    # Pour la régression (output linéaire)
  trace = FALSE,    # Pour désactiver les sorties de progression
  tuneLength = 5    # Essayer 5 combinaisons différentes de paramètres
)

# Afficher les résultats du modèle entraîné
print(annmodelWQI)
annmodelWQI$bestTune
# Faire des prédictions sur les données de test
predictions_ANN <- annmodelWQI %>% predict(test.data)
head(predictions_ANN)
# Évaluer la performance du modèle sur les données de test
results_ANN <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_ANN
)

# Calculer le RMSE (Root Mean Squared Error) pour évaluer les performances
rmse <- sqrt(mean((results_ANN$Observed - results_ANN$Predicted)^2))
# Afficher les résultats
print(results_ANN)
print(paste("RMSE:", rmse))
RMSE(predictions_ANN, test.data$WQI)
MAE(predictions_ANN, test.data$WQI)
R2(predictions_ANN, test.data$WQI)
write.xlsx(results_ANN, file="C:/Users/moste/OneDrive/Bureau/results_ANN.xlsx")
###XGBOOST
library(tidyverse)
library(caret)
library(xgboost)
set.seed(123)  # Fixer la graine pour la reproductibilité
training.samples <- base1$WQI %>%
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- base1[training.samples, ]
test.data <- base1[-training.samples, ]
train_control <- trainControl(method = "cv", number = 10)
tune_grid <- expand.grid(
  nrounds = c(50, 100),          # Nombre d'arbres (réduit à 2 valeurs)
  max_depth = c(3, 6),           # Profondeur maximale des arbres (réduit à 2 valeurs)
  eta = c(0.1, 0.3),             # Taux d'apprentissage (réduit à 2 valeurs)
  gamma = c(0, 1),               # Terme de régularisation pour les branches (réduit à 2 valeurs)
  colsample_bytree = c(0.7),     # Proportion des colonnes échantillonnées (réduit à 1 valeur)
  min_child_weight = c(1, 5),    # Poids minimum pour les enfants (réduit à 2 valeurs)
  subsample = c(0.7)             # Proportion des échantillons utilisés pour construire chaque arbre (réduit à 1 valeur)
)

# Ajuster le modèle XGBoost avec les hyperparamètres définis dans la grille
xgbmodelWQI <- train(
  WQI ~ .,                         # Modéliser WQI en fonction de toutes les autres variables
  data = train.data,               # Données d'entraînement
  method = "xgbTree",              # Utiliser XGBoost avec des arbres de décision
  trControl = train_control,       # Validation croisée à 10 plis
  tuneGrid = tune_grid             # Grille d'hyperparamètres à tester
)
# Best tuning parameter mtry
xgbmodelWQI$bestTune
# Make predictions on the test data
predictions_XGB <- xgbmodelWQI %>% predict(test.data)
head(predictions_XGB)
# Compute the average prediction error RMSE
RMSE(predictions_XGB, test.data$WQI)
MAE(predictions_XGB, test.data$WQI)
R2(predictions_XGB, test.data$WQI)
results_XGB <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_XGB
)
head(results_XGB)
write.xlsx(results_XGB, file="C:/Users/moste/OneDrive/Bureau/results_XGB.xlsx")
###KNN NEIGHBORS###
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
model_KNN<-model <- train(
  WQI~., data = train.data, method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center","scale"),
  tuneLength = 10
)
# Plot model error RMSE vs different values of k
plot(model_KNN)
# Best tuning parameter k that minimize the RMSE
model_KNN$bestTune
# Make predictions on the test data
predictions_KNN <- model_KNN %>% predict(test.data)
head(predictions_KNN)
# Compute the prediction error RMSE
RMSE(predictions_KNN, test.data$WQI)
MAE(predictions_KNN, test.data$WQI)
R2(predictions_KNN, test.data$WQI)
results_KNN <- data.frame(
  Observed = test.data$WQI,
  Predicted = predictions_KNN
)
head(results_KNN)
write.xlsx(results_KNN, file="C:/Users/moste/OneDrive/Bureau/results_KNN.xlsx")
###Predicted and Observed###
XGBP<-ggplot(results_XGB, aes(y=Predicted, x=Observed))+
  geom_point()+
  geom_smooth(colour="red", method="lm", fill="red") +
  ylab("Predicted WQI")+
  xlab("Actual WQI") +
  theme_classic()+
  annotate("text", x = 30, y = 80, label = "(R²=0.97)")
ANannresultsWQI  
R2(predictions_KNN, test.data$WQI)
R2(predictions_XGB, test.data$WQI)
R2(predictions_RF, test.data$WQI)
R2(predictions_DT, test.data$WQI)
R2(predictions_ANN, test.data$WQI)




###DIAGRAM 
# Calculer l'écart-type et le coefficient de corrélation pour chaque modèle
std_dev_DT <- sd(predictions_DT)
corr_coef_DT <- cor(predictions_DT, test.data$WQI)

std_dev_RF <- sd(predictions_RF)
corr_coef_RF <- cor(predictions_RF, test.data$WQI)

std_dev_XGB <- sd(predictions_XGB)
corr_coef_XGB <- cor(predictions_XGB, test.data$WQI)

std_dev_KNN <- sd(predictions_KNN)
corr_coef_KNN <- cor(predictions_KNN, test.data$WQI)

std_dev_ANN <- sd(predictions_ANN)
corr_coef_ANN <- cor(predictions_ANN, test.data$WQI)
# Créer le diagramme de Taylor sans ajuster les limites
taylor.diagram(
  c(std_dev_DT, std_dev_RF, std_dev_XGB, std_dev_KNN, std_dev_ANN),
  c(corr_coef_DT, corr_coef_RF, corr_coef_XGB, corr_coef_KNN, corr_coef_ANN),
  pch=NA, col=NA,
  add=FALSE,
  main="Diagramme de Taylor"
)

# Ajuster les limites des axes après la création du diagramme
par(new=TRUE)
plot(0, 0, type="n", xlim=c(0, 30), ylim=c(0, 30), xlab="", ylab="", axes=FALSE)
axis(1, at=c(0, 7.5, 15, 22.5, 30))
axis(2, at=c(0, 7.5, 15, 22.5, 30))

# Ajouter les points pour chaque modèle
points(std_dev_DT, corr_coef_DT, pch=1, col="red")
points(std_dev_RF, corr_coef_RF, pch=2, col="blue")
points(std_dev_XGB, corr_coef_XGB, pch=3, col="green")
points(std_dev_KNN, corr_coef_KNN, pch=4, col="purple")

# Ajouter une légende
legend("topright", legend=c("DT", "RF", "XGB", "KNN"), pch=1:4, col=c("red", "blue", "green", "purple"))
###taylor diagram###
library(chron)
library(lattice)
library(ggplot2)
library(plotrix)
library(graphics)
###SUPER###
library(openair)
library(plorix)
RMSE(predictions_KNN, test.data$WQI)
RMSE(predictions_DT, test.data$WQI)
RMSE(predictions_RF, test.data$WQI)
RMSE(predictions_ANN, test.data$WQI)
RMSE(predictions_XGB, test.data$WQI)

std_dev_DT <- sd(predictions_DT)
corr_coef_DT <- cor(predictions_DT, test.data$WQI)

std_dev_RF <- sd(predictions_RF)
corr_coef_RF <- cor(predictions_RF, test.data$WQI)

std_dev_XGB <- sd(predictions_XGB)
corr_coef_XGB <- cor(predictions_XGB, test.data$WQI)

std_dev_KNN <- sd(predictions_KNN)
corr_coef_KNN <- cor(predictions_KNN, test.data$WQI)

std_dev_ANN <- sd(predictions_ANN)
corr_coef_ANN <- cor(predictions_ANN, test.data$WQI)


# Les données Observées (observed) et les prédictions pour chaque modèle (pred_*)
observed <- test.data$WQI

# Diagramme de Taylor
taylor.diagram(observed, predictions_DT, add=FALSE, col="red", pch=19, main="Taylor Diagram", pos.cor=TRUE)
taylor.diagram(observed, predictions_RF, add=TRUE, col="blue", pch=19)
taylor.diagram(observed, predictions_ANN, add=TRUE, col="green", pch=19)
taylor.diagram(observed, predictions_XGB, add=TRUE, col="purple", pch=19)
taylor.diagram(observed, predictions_KNN, add=TRUE, col="orange", pch=19)

# Ajouter une légende
legend("topright", legend=c("DT", "RF", "ANN", "XGB", "KNN"), col=c("red", "blue", "green", "purple", "orange"), pch=19)
sd(base1$WQI)


###STUCKING###
# Créer un dataframe avec les prédictions des modèles
stacking_data <- data.frame(
  DT = predictions_DT,
  RF = predictions_RF,
  ANN = predictions_ANN,
  XGB = predictions_XGB,
  KNN = predictions_KNN,
  Observed = test.data$WQI
)

# Entraîner le méta-modèle
meta_model <- lm(Observed ~ ., data = stacking_data)

# Faire des prédictions avec le méta-modèle
meta_predictions <- predict(meta_model, stacking_data)

# Évaluer la performance
meta_rmse <- RMSE(meta_predictions, test.data$WQI)
meta_corr <- cor(meta_predictions, test.data$WQI)
meta_sd <- sd(meta_predictions)

meta_rmse
meta_corr
meta_sd
###comment créer une équation et puis l'importer sur un fichier doc##
library(officer)
library(rmarkdown)
# Créer un document Word
doc <- read_docx()

# Ajouter des équations
doc <- doc %>%
  body_add_par("RMSE = sqrt(1/n * Σ(yi - ŷi)²)", style = "Normal") %>%
  body_add_par("MAE = 1/n * Σ |yi - ŷi|", style = "Normal") %>%
  body_add_par("R² = 1 - Σ(yi - ŷi)² / Σ(yi - ȳ)²", style = "Normal")

# Enregistrer le document
print(doc, target = "equations.docx")
